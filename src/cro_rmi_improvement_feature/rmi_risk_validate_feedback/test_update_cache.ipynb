{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "env_path = \"/Users/ford/Documents/coding/confidential/.env\"\n",
    "load_dotenv(env_path)\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "assert api_key, \"API key is missing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM String used for caching: {\"id\": [\"langchain\", \"chat_models\", \"openai\", \"ChatOpenAI\"], \"kwargs\": {\"model_name\": \"gpt-4o-mini\", \"openai_api_key\": {\"id\": [\"OPENAI_API_KEY\"], \"lc\": 1, \"type\": \"secret\"}, \"temperature\": 0.7}, \"lc\": 1, \"name\": \"ChatOpenAI\", \"type\": \"constructor\"}---[('stop', 'give me random number from 1-1000')]\n",
      "\n",
      "--- First request (should trigger LLM call and cache) ---\n",
      "Cache lookup before first call: [Generation(text='Sure! Here’s a random number between 1 and 1000: **472**.')]\n",
      "Response 1 (LLM Output): Here’s a random number for you: **473**.\n",
      "Cache lookup after first call: [Generation(text='Sure! Here’s a random number between 1 and 1000: **472**.')]\n",
      "\n",
      "--- Second request (should be served from cache) ---\n",
      "Response from Cache: Here’s a random number for you: **473**.\n",
      "Cache lookup after second call: [Generation(text='Sure! Here’s a random number between 1 and 1000: **472**.')]\n",
      "Verification: Second response matched the first (served from cache).\n",
      "\n",
      "--- Forcing re-request and attempting to update cache ---\n",
      "Caching temporarily disabled.\n",
      "Performing forced re-request to get a NEW result...\n",
      "Response 2 (New LLM Output): Sure! Here’s a random number between 1 and 1000: **472**.\n",
      "Caching re-enabled.\n",
      "Cache lookup BEFORE manual update: [Generation(text='Sure! Here’s a random number between 1 and 1000: **472**.')]\n",
      "Cache successfully updated using cache.update().\n",
      "Cache lookup AFTER manual update: [Generation(text='Sure! Here’s a random number between 1 and 1000: **472**.')]\n",
      "\n",
      "--- Subsequent request (Testing if llm.invoke() picks up the update) ---\n",
      "Subsequent Response (from llm.invoke()): Here’s a random number for you: **473**.\n",
      "Subsequent request verification: Still returned the OLD result (manual update NOT reflected immediately by llm.invoke()).\n",
      "Note: While cache.lookup() shows the update, llm.invoke()'s behavior right after manual update can be inconsistent.\n",
      "The update IS persistent in the DB. The *next* time this script runs, the initial cache lookup should yield the new value.\n",
      "\n",
      "Cache data is stored in: .langchain_test_update_cache.db\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_core.globals import set_llm_cache\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.outputs import Generation\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    ")  # Although not directly used for updating, good to have\n",
    "from langchain_community.cache import SQLiteCache\n",
    "from typing import Sequence  # Import Sequence for type hinting\n",
    "\n",
    "# Define the path for the SQLite database file\n",
    "DB_FILE = \".langchain_test_update_cache.db\"\n",
    "\n",
    "# Optional: Remove the database file at the beginning to start with a clean cache\n",
    "# This is useful for consistent testing to ensure you see cache misses initially\n",
    "# if os.path.exists(DB_FILE):\n",
    "#     os.remove(DB_FILE)\n",
    "#     print(f\"Removed existing database file: {DB_FILE}\")\n",
    "\n",
    "# Assume you have your LLM initialized\n",
    "# Using gpt-3.5-turbo for faster response times during testing\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.7)\n",
    "\n",
    "# Set up your SQLite cache\n",
    "cache = SQLiteCache(database_path=DB_FILE)\n",
    "set_llm_cache(cache)\n",
    "\n",
    "# Define your prompt\n",
    "prompt = \"give me random number from 1-1000\"\n",
    "\n",
    "# Get the llm_string *before* any calls, it should be consistent\n",
    "# Note: _get_llm_string might be considered internal, but useful for debugging cache keys\n",
    "llm_string = llm._get_llm_string(prompt)\n",
    "print(f\"LLM String used for caching: {llm_string}\")\n",
    "\n",
    "# --- First run (caches the result) ---\n",
    "print(\"\\n--- First request (should trigger LLM call and cache) ---\")\n",
    "print(\n",
    "    \"Cache lookup before first call:\", cache.lookup(prompt, llm_string)\n",
    ")  # Should be None\n",
    "response1 = llm.invoke(prompt)\n",
    "print(\"Response 1 (LLM Output):\", response1.content)\n",
    "print(\n",
    "    \"Cache lookup after first call:\", cache.lookup(prompt, llm_string)\n",
    ")  # Should show the cached result\n",
    "\n",
    "# --- Second run (should be served from cache) ---\n",
    "print(\"\\n--- Second request (should be served from cache) ---\")\n",
    "response_cached = llm.invoke(prompt)\n",
    "print(\"Response from Cache:\", response_cached.content)\n",
    "print(\n",
    "    \"Cache lookup after second call:\", cache.lookup(prompt, llm_string)\n",
    ")  # Should show the same cached result\n",
    "\n",
    "# Verify they are the same\n",
    "if response_cached.content == response1.content:\n",
    "    print(\"Verification: Second response matched the first (served from cache).\")\n",
    "else:\n",
    "    print(\n",
    "        \"Verification: Second response did NOT match the first (unexpected cache miss/different response).\"\n",
    "    )\n",
    "\n",
    "\n",
    "# --- Force re-request and attempt to update cache ---\n",
    "print(\"\\n--- Forcing re-request and attempting to update cache ---\")\n",
    "\n",
    "# 1. Temporarily disable caching globally\n",
    "# Store the old cache to restore it later if needed, though re-creating is safer for state sync\n",
    "old_cache = set_llm_cache(None)\n",
    "print(\"Caching temporarily disabled.\")\n",
    "\n",
    "# 2. Perform the request to get the new result (cache is off, so LLM is called)\n",
    "print(\"Performing forced re-request to get a NEW result...\")\n",
    "new_response_message = llm.invoke(prompt)\n",
    "new_response_text = new_response_message.content\n",
    "print(\"Response 2 (New LLM Output):\", new_response_text)\n",
    "\n",
    "# 3. Re-enable the cache. Crucially, we might need a new instance or re-set\n",
    "# the same instance to ensure Langchain's internal state is updated to use it.\n",
    "# Re-creating and setting seems the most reliable way to force potential state sync.\n",
    "cache = SQLiteCache(database_path=DB_FILE)\n",
    "set_llm_cache(cache)\n",
    "print(\"Caching re-enabled.\")\n",
    "\n",
    "\n",
    "# 4. Manually update the cache with the new result\n",
    "# The value stored must be a Sequence[Generation]. AIMessage can be converted.\n",
    "# Check current cache value BEFORE update\n",
    "print(\n",
    "    \"Cache lookup BEFORE manual update:\", cache.lookup(prompt, llm_string)\n",
    ")  # Should still show the OLD cached value\n",
    "\n",
    "# Prepare the new value in the expected format\n",
    "# Ensure new_response_message is an AIMessage or similar object that has content\n",
    "# If new_response_text was just a string, wrap it like: [Generation(text=new_response_text)]\n",
    "new_return_val: Sequence[Generation] = [Generation(text=new_response_text)]\n",
    "\n",
    "try:\n",
    "    # Use the original llm_string for the update key\n",
    "    cache.update(prompt, llm_string, new_return_val)\n",
    "    print(\"Cache successfully updated using cache.update().\")\n",
    "except Exception as e:\n",
    "    print(f\"Error updating cache: {e}\")\n",
    "\n",
    "# Check cache value AFTER update using the cache object directly\n",
    "print(\n",
    "    \"Cache lookup AFTER manual update:\", cache.lookup(prompt, llm_string)\n",
    ")  # Should now show the NEW cached value\n",
    "\n",
    "\n",
    "# --- Subsequent request (intended to be served from the updated cache) ---\n",
    "print(\"\\n--- Subsequent request (Testing if llm.invoke() picks up the update) ---\")\n",
    "\n",
    "# While cache.lookup() confirms the update, llm.invoke() might not immediately\n",
    "# reflect this manual change in the same script execution. This is the observed\n",
    "# behavior from your output.\n",
    "\n",
    "\n",
    "# Use the same LLM instance and prompt\n",
    "response_subsequent = llm.invoke(prompt)\n",
    "print(\"Subsequent Response (from llm.invoke()):\", response_subsequent.content)\n",
    "\n",
    "# Verification of the *subsequent invoke* result\n",
    "if response_subsequent.content == new_response_text:\n",
    "    print(\n",
    "        \"Subsequent request verification: Returned the NEW result (update reflected immediately).\"\n",
    "    )\n",
    "else:\n",
    "    # This is the outcome you observed and is common due to internal state\n",
    "    print(\n",
    "        \"Subsequent request verification: Still returned the OLD result (manual update NOT reflected immediately by llm.invoke()).\"\n",
    "    )\n",
    "    print(\n",
    "        \"Note: While cache.lookup() shows the update, llm.invoke()'s behavior right after manual update can be inconsistent.\"\n",
    "    )\n",
    "    print(\n",
    "        \"The update IS persistent in the DB. The *next* time this script runs, the initial cache lookup should yield the new value.\"\n",
    "    )\n",
    "\n",
    "\n",
    "print(f\"\\nCache data is stored in: {DB_FILE}\")\n",
    "\n",
    "# To verify the update persisted, re-run this script. The first \"Cache lookup before first call\"\n",
    "# should now show the NEW number obtained in the \"Response 2 (New LLM Output)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Generation(text='Sure! Here’s a random number between 1 and 1000: **472**.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache.lookup(prompt, llm_string=llm_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Here’s a random number for you: **473**.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 17, 'total_tokens': 30, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0392822090', 'id': 'chatcmpl-BPNZE8VVJgicIlgDPdKkbsVILOYNO', 'finish_reason': 'stop', 'logprobs': None}, id='run-9cff66a0-8a8c-40aa-9de7-8e83cd9fab38-0', usage_metadata={'input_tokens': 17, 'output_tokens': 13, 'total_tokens': 30, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
